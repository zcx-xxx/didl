{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4249e-38, 3.0850e-41, 1.4427e-38, 3.0850e-41],\n",
      "        [8.9683e-44, 0.0000e+00, 1.1210e-43, 0.0000e+00],\n",
      "        [1.4280e-38, 3.0850e-41, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "tensor([2, 3, 4, 5], dtype=torch.int32) tensor([2, 3, 4, 5], dtype=torch.int32)\n",
      "tensor([1, 2, 3, 4])\n",
      "[1, 2, 3, 4] tensor([2, 3, 4, 5]) tensor([1, 2, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_339937/4077079391.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  e_tensor = torch.tensor(d_tensor)\n"
     ]
    }
   ],
   "source": [
    "# 创建指定 size 的张量，数据为随机值\n",
    "ts = torch.Tensor(3, 4)\n",
    "print(ts)\n",
    "\n",
    "# 根据列表创建张量，数据类型默认为 float\n",
    "a_tensor = torch.Tensor([1, 2, 3, 4])\n",
    "print(a_tensor)\n",
    "\n",
    "# 创建 int 型的张量\n",
    "b_tensor = torch.IntTensor([1, 2, 3, 4])\n",
    "print(b_tensor)\n",
    "\n",
    "# 以张量创建张量，共享 data\n",
    "c_tensor = torch.IntTensor(b_tensor)\n",
    "b_tensor += 1\n",
    "print(b_tensor, c_tensor)       # tensor([2, 3, 4, 5], dtype=torch.int32) tensor([2, 3, 4, 5], dtype=torch.int32)\n",
    "\n",
    "# 可用 python 列表，numpy 数组，Tensor 张量创建，不可以通过指定大小创建\n",
    "# 这是一个 torch 的方法，上边的 Tensor 是一个类\n",
    "ts1_n = [1, 2, 3, 4]\n",
    "ts1 = torch.tensor(ts1_n)\n",
    "print(ts1)\n",
    "\n",
    "# tensor() 方法不共享data内存，用 is 和 id 判断都不准，tensor() 方法总是对 data 进行一份拷贝\n",
    "a = [1, 2, 3, 4]\n",
    "d_tensor = torch.tensor(a)\n",
    "e_tensor = torch.tensor(d_tensor)\n",
    "d_tensor += 1\n",
    "print(a, d_tensor, e_tensor)        # [1, 2, 3, 4] tensor([2, 3, 4, 5]) tensor([1, 2, 3, 4])\n",
    "# print(id(d_tensor.storage), id(e_tensor.storage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7363, 0.7736, 0.9905],\n",
      "        [0.5889, 0.3096, 0.1179]])\n",
      "tensor([[-0.6578,  0.9412,  1.1411],\n",
      "        [-0.3230, -1.4090,  1.5763]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "# 从均匀分布返回一个随机数填充的张量，区间为[0, 1)\n",
    "tr = torch.rand(2, 3)\n",
    "print(tr)\n",
    "\n",
    "# 从标准正态分布返回一个随机填充的张量，均值为 0，方差为 1\n",
    "trn = torch.randn(2, 3)\n",
    "print(trn)\n",
    "\n",
    "# 返回一个全为 1 的张量\n",
    "t_one = torch.ones(2, 3)\n",
    "t_zero = torch.zeros(2, 3)\n",
    "# 返回一个与输入张量形状一样的张量\n",
    "t_one_1 = torch.ones_like(t_one)\n",
    "t_zero_1 = torch.zeros_like(t_zero)\n",
    "print(t_one)\n",
    "print(t_one_1)\n",
    "print(t_zero)\n",
    "print(t_zero_1)\n",
    "\n",
    "# 用 0 填充自己\n",
    "t_one.zero_()\n",
    "print(t_one)\n",
    "\n",
    "# 改变当前张量的 size，返回的新的张量和当前张量共享 data\n",
    "t_one_view = t_one.view(-1, 2)\n",
    "print(t_one)\n",
    "print(t_one_view)\n",
    "t_one[0][1] = 1\n",
    "print(t_one)\n",
    "print(t_one_view)\n",
    "\n",
    "# 矩阵乘法\n",
    "data_1 = torch.ones(2, 3)\n",
    "data_2 = torch.ones(3, 2)\n",
    "data = torch.mm(data_1, data_2)\n",
    "print(data)\n",
    "\n",
    "# 返回张量所有元素的和\n",
    "print(torch.sum(data))\n",
    "\n",
    "# 计算点积\n",
    "torch.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标量自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>) tensor(2.) None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = x**2 + y\n",
    "\n",
    "# 返回一个新的张量，将 y 从计算图上分离\n",
    "u = y.detach()\n",
    "\n",
    "z = x**2 + u\n",
    "z.backward()\n",
    "print(z, x.grad, u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5332, 0.5752, 0.7980], requires_grad=True)\n",
      "tensor([0.2843, 0.3308, 0.6368], grad_fn=<PowBackward0>)\n",
      "tensor([1.0664, 1.1503, 1.5960], grad_fn=<AddBackward0>)\n",
      "tensor([0.2843, 0.3308, 0.6368], grad_fn=<PowBackward0>) tensor([1.0664, 1.1503, 1.5960])\n",
      "tensor([1.0664, 1.1503, 1.5960], grad_fn=<AddBackward0>) (tensor([2., 2., 2.]),)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x**2\n",
    "print(y)\n",
    "\n",
    "z = x + x\n",
    "print(z)\n",
    "\n",
    "# 只有标量可以调用 backward\n",
    "y.sum().backward()\n",
    "print(y, x.grad)\n",
    "\n",
    "# 为了防止梯度累加，将之前的梯度清零\n",
    "x.grad.zero_()\n",
    "\n",
    "# 以下四种方式等同\n",
    "# z.sum().backward()\n",
    "# z.backward(torch.ones(len(x)))\n",
    "# torch.autograd.backward(z, torch.ones(len(x)))\n",
    "grads = torch.autograd.grad(z, x, torch.ones(len(x)))\n",
    "print(z, grads)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1854e41a69a0e1d0b7c2cbc8cd354d15242291d211e3f09d611ca6203fa6e0d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
